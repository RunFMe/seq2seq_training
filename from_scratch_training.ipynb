{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a62d887-01aa-4980-9220-3bf28c077504",
   "metadata": {},
   "source": [
    "### –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ Transformers Training (50 –±–∞–ª–ª–æ–≤)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d7452-febe-4d25-a9c1-005fcc26b35b",
   "metadata": {},
   "source": [
    "–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ Transformer-based –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–µ–∫—É—â–∏–º –ø—Ä–æ–µ–∫—Ç–æ–º, —Ç–∞–∫ –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–≤–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è. –ï—Å–ª–∏ –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–µ–∫—Ç, —Ç–µ–≥–∏ **TODO** –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ç–º–µ—á–∞—é—Ç, –∫–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞–¥–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å.\n",
    "–í –Ω–æ—É—Ç–±—É–∫–µ –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥—ã. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏(–∫–æ–ª–∏—á–µ—Ç—Å–≤–æ —Å–ª–æ–µ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏ —Ç–¥) –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∞ –≤–∞—à –≤—ã–±–æ—Ä.\n",
    "\n",
    "–í–∞—à –∫–æ–¥ –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–Ω–æ –≤—ã–ª–æ–∂–∏—Ç—å –Ω–∞ –≤–∞—à github, –≤ —Å—Ç—Ä–æ–∫–µ –Ω–∏–∂–µ –¥–∞—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ –Ω–µ–≥–æ. –í –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –±—É–¥—É—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –Ω–æ—É—Ç–±—É–∫–µ, –∫–æ–¥ –Ω—É–∂–µ–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. \n",
    "\n",
    "–û–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞ –Ω–µ –Ω—É–∂–Ω–æ, —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∏ —Ä–∞–±–æ—á–∞—è - —Å–Ω–∏–∂–µ–Ω–∏–µ val_loss, —Ä–æ—Å—Ç bleu_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca1e62-b210-4426-9854-55b652417a4b",
   "metadata": {},
   "source": [
    "### –î–∞–Ω–Ω—ã–µ\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "–ú–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –Ω–∞ –∑–∞–¥–∞—á–µ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e7b9-6538-42b1-96b8-838f2efab4af",
   "metadata": {},
   "source": [
    "#### –°c—ã–ª–∫–∞ –Ω–∞ –≤–∞—à github —Å –ø—Ä–æ–µ–∫—Ç–æ–º(–≤—Å—Ç–∞–≤–∏—Ç—å —Å–≤–æ–π) - https://github.com/runnerup96/pytorch-machine-translation\n",
    "\n",
    "–ù–æ—É—Ç–±—É–∫ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—ã–∫–ª–∞–¥—ã–≤–∞—Ç—å –Ω–∞ –≤–∞—à **google –¥–∏—Å–∫** –∫—É—Ä—Å–∞. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fe8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1b9274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/runfme_default/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jovyan/runfme/triton/translate/datamanip.py:71: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/home/user/conda/envs/runfme_default/lib/python3.11/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datamanip import get_dataset, get_tokenizer,compute_metrics, preprocess_function, DatasetConfig, collate_fn\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer  \n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a389be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c70106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 312726/312726 [00:10<00:00, 29319.69 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9672/9672 [00:00<00:00, 29711.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from functools import partial\n",
    "\n",
    "conf = OmegaConf.load('./cfg_tune_t5.yaml')\n",
    "dataset_cfg = OmegaConf.structured(DatasetConfig(**conf['dataset']))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./my_tokenizer')\n",
    "import datamanip\n",
    "datamanip.tokenizer = tokenizer\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "dataset = get_dataset(dataset_cfg)\n",
    "dataset = dataset.train_test_split(test_size=0.03)\n",
    "tokenized_dataset = dataset.map(partial(preprocess_function, add_eos=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b9a9fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyEncoderDecoderModelForSeq2SeqLM(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-4): 5 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (shared): Embedding(20000, 256)\n",
       "  (lm_head): Linear(in_features=256, out_features=20000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from model import MyEncoderDecoderModelForSeq2SeqLM, ModelConfig\n",
    "\n",
    "model_new = MyEncoderDecoderModelForSeq2SeqLM(\n",
    "    ModelConfig(vocab_size=len(tokenizer), pad_token=pad_token_id, eos_token=eos_token_id)\n",
    ")\n",
    "model_new.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba3c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "EPOCH_NUM = 50\n",
    "train_dl = DataLoader(tokenized_dataset['train'], batch_size=200, shuffle=True, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(tokenized_dataset['test'], batch_size=200, shuffle=False, collate_fn=collate_fn)\n",
    "optimizer = optim.Adam(model_new.parameters(), lr=3e-4)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, len(train_dl) * EPOCH_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b2b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrunfme\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/runfme/triton/translate/wandb/run-20240511_195113-gil261ug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/runfme/uncategorized/runs/gil261ug' target=\"_blank\">chocolate-sound-194</a></strong> to <a href='https://wandb.ai/runfme/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/runfme/uncategorized' target=\"_blank\">https://wandb.ai/runfme/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/runfme/uncategorized/runs/gil261ug' target=\"_blank\">https://wandb.ai/runfme/uncategorized/runs/gil261ug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1564 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 743/1564 [00:17<00:18, 44.89it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7efcc4f18b90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/conda/envs/runfme_default/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1564/1564 [00:36<00:00, 43.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval crossentropy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:00, 67.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 85/1564 [00:02<00:35, 42.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/home/user/conda/envs/runfme_default/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/user/conda/envs/runfme_default/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"]= '844ae81dd9f3dac384cf5cdb478d4e939ed71fa1'\n",
    "\n",
    "wandb.init()\n",
    "\n",
    "step = 0\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    print('Train')\n",
    "    for i, train_batch in enumerate(tqdm(train_dl)):\n",
    "        res = model_new(**{k: v.cuda() for k,v in train_batch.items() if k in ['input_ids', 'labels','attention_mask']})\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        res.loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print(res.loss)\n",
    "        if step%50 == 0:\n",
    "            wandb.log({'lr': optimizer.param_groups[0]['lr'], 'loss': res.loss.item()}, step)\n",
    "        step+=1\n",
    "        \n",
    "    print('Eval crossentropy')\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, test_batch in tqdm(enumerate(test_dl)):\n",
    "            res = model_new(**{k: v.cuda() for k,v in test_batch.items() if k in ['input_ids', 'labels','attention_mask']})\n",
    "            losses.append(res.loss.item())\n",
    "    wandb.log({'val_loss': np.mean(losses)}, step)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0  or epoch == EPOCH_NUM - 1:\n",
    "        print('Eval bleu')\n",
    "        preds = []\n",
    "        targets = [] \n",
    "        with torch.no_grad():\n",
    "            for test_sample in tqdm(tokenized_dataset['test']):\n",
    "                targets.append(test_sample['labels'])\n",
    "                result = model_new.generate(torch.tensor(test_sample['input_ids']).cuda(), max_length=30)\n",
    "                preds.append(result.cpu())\n",
    "                \n",
    "        max_target_len = max([len(seq) for seq in targets])\n",
    "        padded_targets = [seq + [-100] * (max_target_len - len(seq)) for seq in targets]\n",
    "        bleu_score = compute_metrics((torch.stack(preds),torch.tensor(padded_targets)))        \n",
    "        print(f'BLEU: {bleu_score}')\n",
    "        wandb.log({'bleu_score': bleu_score}, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf6ea75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_new.load_state_dict(torch.load('./from_scratch_10ep.pt'))\n",
    "# torch.save(model_new.state_dict(), './from_scratch_10ep.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41063d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eng: He chose not to run for the presidential election.\n",
      "Reference: –û–Ω —Ä–µ—à–∏–ª –Ω–µ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —É—á–∞—Å—Ç–∏—è –≤ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç—Å–∫–∏—Ö –≤—ã–±–æ—Ä–∞—Ö.\n",
      "Predicted: –û–Ω —Ä–µ—à–∏–ª –Ω–µ –Ω–∞ –≤—ã–±–æ—Ä–∞—Ö –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞.\n",
      "Eng: I don't feel sorry for her.\n",
      "Reference: –ú–Ω–µ –µ—ë –Ω–µ –∂–∞–ª–∫–æ.\n",
      "Predicted: –ú–Ω–µ –Ω–µ –∂–∞–ª–∫–æ. –Ø —á—Ç–æ –≤–∞–º –Ω–µ –∂–∞–ª–∫–æ. —Å–æ—á—É–≤—Å—Ç–≤—É—é –µ—ë.\n",
      "Eng: I think that Tom won't go to Boston with Mary.\n",
      "Reference: –Ø –¥—É–º–∞—é, –¢–æ–º –Ω–µ –ø–æ–µ–¥–µ—Ç —Å –ú—ç—Ä–∏ –≤ –ë–æ—Å—Ç–æ–Ω.\n",
      "Predicted: –î—É–º–∞—é, –¢–æ–º –Ω–µ –ø–æ–µ–¥–µ—Ç –≤ –ë–æ—Å—Ç–æ–Ω —Å –ú—ç—Ä–∏.\n",
      "Eng: Please bring us two cups of coffee.\n",
      "Reference: –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–∏–Ω–µ—Å–∏—Ç–µ –Ω–∞–º –¥–≤–µ —á–∞—à–∫–∏ –∫–æ—Ñ–µ.\n",
      "Predicted: –ó–∞–π–¥–∏—Ç–µ –º—ã –∫–æ—Ñ–µ –∫–æ—Ñ–µ –∫–æ—Ñ–µ.\n",
      "Eng: Tom is Mary's half-brother.\n",
      "Reference: –¢–æ–º ‚Äî –µ–¥–∏–Ω–æ–∫—Ä–æ–≤–Ω—ã–π –±—Ä–∞—Ç –ú—ç—Ä–∏.\n",
      "Predicted: –¢–æ–º –≤–µ–¥—å –ø–æ–¥–∞—Å—Ç –ú—ç—Ä–∏ - –ø–ª–µ–º—è–Ω–Ω–∏—Ü–µ –¥–∞–º –¥—Ä—É–≥ —É –ú—ç—Ä–∏.\n",
      "Eng: The rat made a hole in the wall.\n",
      "Reference: –ö—Ä—ã—Å–∞ –ø—Ä–æ–≥—Ä—ã–∑–ª–∞ –¥—ã—Ä—É –≤ —Å—Ç–µ–Ω–µ.\n",
      "Predicted: –í —Å—Ç–µ–Ω–µ –≤–∏—Å–∫–∏ –≥–æ–ª—ã–º–∏.\n",
      "Eng: I'm a bus driver.\n",
      "Reference: –Ø –≤–æ–¥–∏—Ç–µ–ª—å –∞–≤—Ç–æ–±—É—Å–∞.\n",
      "Predicted: –Ø –∞–≤—Ç–æ–±—É—Å –≤–æ–¥–∏—Ç–µ –≤–æ–¥–∏—Ç–µ–ª—å –∞–≤—Ç–æ–±—É—Å–∞. –Ø –≤–æ–¥–∏—Ç–µ–ª—å –∞–≤—Ç–æ–±—É—Å–∞.\n",
      "Eng: Can I have this film developed?\n",
      "Reference: –ù–µ –º–æ–≥–ª–∏ –±—ã –≤—ã –ø—Ä–æ—è–≤–∏—Ç—å —ç—Ç—É –ø–ª—ë–Ω–∫—É?\n",
      "Predicted: –£ –º–µ–Ω—è –µ—Å—Ç—å –∫–∏–Ω–æ –ø–ª–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è –º–æ–≥ –±—ã —Ä–∞–∑–¥—É–º—ã?\n",
      "Eng: Have you told anyone about this?\n",
      "Reference: –í—ã –æ–± —ç—Ç–æ–º –∫–æ–º—É-–Ω–∏–±—É–¥—å –≥–æ–≤–æ—Ä–∏–ª–∏?\n",
      "Predicted: –ö—Ç–æ-–Ω–∏–±—É–¥—å –≥–æ–≤–æ—Ä–∏–ª –≤–∞–º –æ–± —ç—Ç–æ–º —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–ª?\n",
      "Eng: I was hoping you'd ask.\n",
      "Reference: –Ø –Ω–∞–¥–µ—è–ª–∞—Å—å, —á—Ç–æ –≤—ã —Å–ø—Ä–æ—Å–∏—Ç–µ.\n",
      "Predicted: –Ø –Ω–∞–¥–µ—è–ª—Å—è, —á—Ç–æ –≤—ã —Å–ø—Ä–æ—Å–∏—Ç–µ. –í—ã —Å–ø—Ä–æ—Å–∏—Ç–µ.\n"
     ]
    }
   ],
   "source": [
    "for i in dataset['test'].select(range(10)):\n",
    "    source, target = i['source'], i['target']\n",
    "    inputs = tokenizer(\n",
    "        source,\n",
    "        return_tensors='pt',\n",
    "        max_length=256,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Get correct sentence ids.\n",
    "    corrected_ids = model_new.generate(\n",
    "        inputs.input_ids[0].cuda(),\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    # Decode.\n",
    "    res = tokenizer.decode(\n",
    "        corrected_ids,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(f'Eng: {source}')\n",
    "    print(f'Reference: {target}')\n",
    "    print(f'Predicted: {res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11935b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
